Created by Andreas Oven Aalsaunet

Task 1

The syntactic rules are (rules|tag count|rule count| conditional probabilities):
|     Rules      | T | R | P(R|C)|
|S  -> NP VP     | 2 | 2 |   1   |
|NP -> NNP       | 6 | 4 |  2/3  |
|NP -> DT NN     | 6 | 2 |  1/3  |
|VP -> VBD NP PP | 2 | 1 |  1/2  |
|VP -> VBD NP NP | 2 | 1 |  1/2  |
|PP -> P NP      | 1 | 1 |   1   |

The lexical rules are (rules|tag count|rule count| conditional probabilities):
|     Rules      | T | R | P(R|C)|
|NNP -> Frodo    | 4 | 2 |  1/2  |
|NNP -> Sam      | 4 | 2 |  1/2  |
|DT  -> the      | 2 | 2 |   1   |
|NN  -> ring     | 2 | 2 |   1   |
|P   -> to       | 1 | 1 |   1   |
|VBD -> gave     | 2 | 2 |   1   |

Task 3A

TODO: Correct this and include that edges also gets added through the fundamental rule.

When an edge is created it is stored in the agenda where it is later retrieved and
added to the chart. A new edge gets added to the agenda from two places in the parse-function.

I. Initialization 
In the initialization step of the parse-function an edge is created for each
word and category combination of the input sequence. If e.g. the word "flies" appear in the
input sequence, and we use the toy.mrg-file as our grammar, two edges is created from
this word: One representing the word as an plural noun and one representing the the word as a
present verb.

When an edge is a created for a word-category pair at this point in the function, it is given
the following properties:

from i      = Start position of the edge where i is the position of the word
     	      in the input string 
to i+1      = End postion of the edge. Since we are creating edges for individual words at this
       	      point the vertix/substring length is 1, thus end = start + 1
category    = the category of the lexeme struct that was retrieved when calling
	      (get-lexeme word grammar)
daughters   = A list of lexical edges that is passive from the start.
probability = The probability of the lexeme struct that was retrieved when calling
	      (get-lexeme word grammar)

In order for this to happen one condition must be met: The word must appear in the
training data. If no lexeme(s) is returned when calling the get-lexemes with the word
and the grammar struct (that stores the training data), the parse function returns/aborts
and returns nil.

II. Main Parser Loop
In the main loop of the parse-function an edge is created if there are no equivalent
edges in the chart yet. If this occurres the function adds the edge, applies the fundamental
rule and predicts new edges which is also added to the agenda.

When an edge is created at this point in the function, it is given the following properties:
from        = The same from-value as the host edge.
to          = The same to-value as the host edge.
category    = The non-terminal on the left hand side of the rule that is derived from
	      calling the function rules-starting-in with the host edge's
	      category and the current grammar.
daughters   = A list, initially containing the host edge.
unanalyzed  = The non-terminal(s), except the first, on the right hand side of the
	      rule that is derived from calling the function rules-starting-in
	      with the host edge's category and the current grammar
probability = The probability of the rule that is derived from
	      calling the function rules-starting-in with the host edge's
	      category and the current grammar.

As stated above these events are triggered if there are no equivalent edges in the chart.
If there are equivalent edges, these extra "prediciton-edges" is not created.


TASK 4A

The viterbi function's task is to find the most probable parse tree in
our parse forest (alternate trees) and return it. The packed forrest is here represented as
a single edge, which can contained multiple packed alternatives. 

In our implementation the function starts out by checking if the edge, given as a argument to
the function, has values stored in its cache. The catch is a variable in each of the edge
objects that stores results to avoid recalculations (dynamic programming).
If the edge variable is empty (nil) it means that the calculations have yet to be done,
and the function continues. If the cache contains values the rest of the statements in the
function is skipped.

If edge's cache is empty the function next checks if the edge has daughters (i.e. edge's
daughter is not nil). If the edge has no daughters the edge-cache is simply set to point to
the edge itself (to avoid doing this check twice).
If the edge does have daughters, the function goes into a two-level loop,
which both contains recursive calls. 

At the start of the first loop, the probability of the edge is set to itself + the result
of the second level loop. In the second level loop each of the edge's daughters are
iterated through and for each iteration the loop sums the result of calling
"(viterbi daughter)" on each of the egde's daughters (recursive calls). Once the second
level loop has finished the edge's probability is thus the sum of its own probability and all
its daughters probabilites.

The first level loop continues by iterating through all the edge's alternates, which is
other complete trees in the parse forrest, also represented as edges. For each of these
alternates the viterbi-function is called with the alternate edge (recursive calls) and their
probabilities is calculated in the same manner as above. For each of these resulting
probabilities, the loops checks if any of these are higher than the original edge's
probability. If this is the case the edge-probability is set to that probability and the
edge's daughters are replaced with the daughters of the alternate, effectivly replacing
the parse tree with the more probable one (the alternate). When the loops have finished the
edge, which is now the most probable parse tree, is returned.

TASK 4B

Conceptually both our Hidden Markov Model implementation and our Chart Parser implementation
uses viterbi the same way: To continously store calcultations and use these instead of doing
the same calculations several times (dynamic programming), and to keep a record of
backpointers to traverse our model and finding the the most probable path or alternative.
Even though both our programs does this the implementations are different in many ways.
In our implementations of the HMM we used two trellises that was very similar to the
conceptual two dimensional matrices: One that stored the (forward) path probability
(i.e the probability from going from one state to another) and one that stored the
backpointers, which pointed out which cell on the previous row that had the
maximum probability for resulting in the current one (which enable us to find the highest
probability path).

In our chart there are no such trellises, this information is instead saved within the
edge structures themself. Previously calculated information, which in HMM's case was stored
in the (forward) path probability trellis, is in the Chart Parser's case stored within
the relevant edge's cache-variable. Previously calculated most probable edges are thus
stored in this cache-slot. Backpointer information, which in HMM's case was stored in
the backpointer trellis, is stored in the daugters slot of the edge. As
our parser takes a bottom-up approach, and thus finds the lowest elements in the trees
first, and builds the trees "upwards" the daughters points to the edges "below" the
current edge and is thus also a backpointer of sorts.

The columns that stored states in the HMM are in this implementation replaced by list of
alternate edges that is kept in each of the edges. As our HMM implementation did for all
states, this implementation does through the edge struct: It compares the previously
computed max probability of each transition (from one edge to another) and updates the max
when appriate, in the same way our HMM did with state transitions.
 
